{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6장 웹 스크레이핑"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 웹 스크레이핑을 위한 기본 지식"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.1 웹 스크레이핑의 과정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.2 웹 스크레이핑 시 주의 사항"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 주요 주의 사항"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 웹 사이트 이용 규약"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.3 웹 데이터의 요청과 응답 과정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.4 웹 페이지 언어(HTML) 구조"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[6장: 215페이지]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile C:\\myPyScraping\\data\\ch06\\HTML_example.html         \n",
    "<!doctype html>\n",
    "<html>\n",
    " <head>\n",
    "  <meta charset=\"utf-8\">\n",
    "  <title>이것은 HTML 예제</title>\n",
    " </head>\n",
    " <body>\n",
    "  <h1>출간된 책 정보</h1>\n",
    "  <p id=\"book_title\">이해가 쏙쏙 되는 파이썬</p>\n",
    "  <p id=\"author\">홍길동</p>\n",
    "  <p id=\"publisher\">위키북스 출판사</p>\n",
    "  <p id=\"year\">2018</p>\n",
    " </body>\n",
    "</html>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[6장: 216페이지]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile C:/myPyScraping/data/ch06/HTML_example2.html \n",
    "<!doctype html>\n",
    "<html>\n",
    " <head>\n",
    "  <meta charset=\"utf-8\">\n",
    "  <title>이것은 HTML 예제</title>\n",
    " </head>\n",
    " <body>\n",
    "  <h1>출간된 책 정보</h1>\n",
    "  <p>이해가 쏙쏙 되는 파이썬</p>\n",
    "  <p>홍길동</p>\n",
    "  <p>위키북스 출판사</p>\n",
    "  <p>2018</p>\n",
    "  </body>\n",
    "</html>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.5 웹 페이지의 소스 가져오기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 웹 브라우저로 웹 페이지 소스 보기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  requests 라이브러리 활용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  GET 메서드로 웹 사이트의 소스 가져오기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[6장: 220페이지]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "r = requests.get(\"https://www.google.co.kr\")\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r.status_code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[6장: 221페이지]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r.text[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r.headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "html = requests.get(\"https://www.google.co.kr\").text\n",
    "html[0:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.6 웹 페이지의 소스 분석하고 처리하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 데이터 찾고 추출하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[6장: 222페이지]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# 테스트용 html 소스\n",
    "html = \"\"\"<html><body><div><span>\\\n",
    "        <a href=http://www.naver.com>naver</a>\\\n",
    "        <a href=https://www.google.com>google</a>\\\n",
    "        <a href=http://www.daum.net/>daum</a>\\\n",
    "        </span></div></body></html>\"\"\" \n",
    "\n",
    "# BeautifulSoup를 이용해 HTML 소스를 파싱\n",
    "soup = BeautifulSoup(html, 'lxml') \n",
    "soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[6장: 224페이지]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find('a').get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find('a')['href'] # soup.find('a').get('href') 도 동일"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[6장: 225페이지]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find_all('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[x.get_text() for x in soup.find_all('a')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# 테스트용 HTML 코드\n",
    "html2 = \"\"\"\n",
    "<html>\n",
    " <head>\n",
    "  <title>작품과 작가 모음</title>\n",
    " </head>\n",
    " <body>\n",
    "  <h1>책 정보</h1>\n",
    "  <p id=\"book_title\">토지</p>\n",
    "  <p id=\"author\">박경리</p>\n",
    "  \n",
    "  <p id=\"book_title\">태백산맥</p>\n",
    "  <p id=\"author\">조정래</p>\n",
    "\n",
    "  <p id=\"book_title\">감옥으로부터의 사색</p>\n",
    "  <p id=\"author\">신영복</p>\n",
    "  </body>\n",
    "</html>\n",
    "\"\"\" \n",
    "\n",
    "soup2 = BeautifulSoup(html2, \"lxml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[6장: 226페이지]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup2.title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "soup2.body"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[6장: 227페이지]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup2.body.h1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup2.p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "soup2.find_all('p')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[6장: 228페이지]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup2.find('p', {\"id\":\"book_title\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup2.find('p', {\"id\":\"author\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup2.find_all('p', {\"id\":\"book_title\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup2.find_all('p', {\"id\":\"author\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "soup2 = BeautifulSoup(html2, \"lxml\")\n",
    "\n",
    "book_titles = soup2.find_all('p', {\"id\":\"book_title\"})\n",
    "authors = soup2.find_all('p', {\"id\":\"author\"})\n",
    "\n",
    "for book_title, author in zip(book_titles, authors):\n",
    "    print(book_title.get_text() + '/' + author.get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[6장: 230페이지]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup2.select_one('body h1') # body 내의 h1 태그를 갖는 최초의 요소 찾기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup2.select('body h1') # body 내의 h1 태그를 갖는 모든 요소 찾기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup2.select_one('body p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup2.select('body p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "soup2.select('p')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[6장: 231페이지]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup2.select('p#book_title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup2.select('p#author')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile C:/myPyScraping/data/ch06/HTML_example_my_site.html \n",
    "<!doctype html>\n",
    "<html>\n",
    "  <head>\n",
    "    <meta charset=\"utf-8\">\n",
    "    <title>사이트 모음</title>\n",
    "  </head>\n",
    "  <body>\n",
    "    <p id=\"title\"><b>자주 가는 사이트 모음</b></p>\n",
    "    <p id=\"contents\">이곳은 자주 가는 사이트를 모아둔 곳입니다.</p>\n",
    "    <a href=\"http://www.naver.com\" class=\"portal\" id=\"naver\">네이버</a> <br>\n",
    "    <a href=\"https://www.google.com\" class=\"search\" id=\"google\">구글</a> <br>\n",
    "    <a href=\"http://www.daum.net\" class=\"portal\" id=\"daum\">다음</a> <br>\n",
    "    <a href=\"http://www.nl.go.kr\" class=\"government\" id=\"nl\">국립중앙도서관</a>\n",
    "  </body>\n",
    "</html>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[6장: 232페이지]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('C:/myPyScraping/data/ch06/HTML_example_my_site.html', encoding='utf-8')\n",
    "\n",
    "html3 = f.read()\n",
    "f.close()\n",
    "\n",
    "soup3 = BeautifulSoup(html3, \"lxml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "soup3.select('a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[6장: 233페이지]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup3.select('a.portal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup3.select_one('a').get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[x.get_text() for x in soup3.select('a')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 웹 브라우저의 요소 검사"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[6장: 235페이지]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup3.select('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "soup3.select('a.portal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[6장: 236페이지]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup3.select('a#naver')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup3.select('a#naver.portal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup3.select('a.portal#naver')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.7 웹 사이트 주소에 부가 정보 추가하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 웹 사이트 주소에 경로 추가하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[6장: 237페이지]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"https://api.github.com/\"\n",
    "sub_dir = \"events\"\n",
    "url = base_url + sub_dir\n",
    "print(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "base_url = \"https://api.github.com/\"\n",
    "sub_dirs = [\"events\", \"user\", \"emails\"]\n",
    "\n",
    "for sub_dir in sub_dirs:\n",
    "    url_dir = base_url + sub_dir\n",
    "    r = requests.get(url_dir)\n",
    "    print(r.url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 웹 사이트 주소에 매개변수 추가하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[6장: 238페이지]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "where_value = 'nexearch' \n",
    "sm_value = 'top_hty'\n",
    "fbm_value = 1\n",
    "ie_value = 'utf8'\n",
    "query_value = 'python'\n",
    "\n",
    "base_url = \"https://search.naver.com/search.naver\"\n",
    "parameter = \"?where={0}&sm={1}&fbm={2}&ie={3}&query={4}\".format(where_value, sm_value, fbm_value, ie_value, query_value)\n",
    "url_para = base_url + parameter\n",
    "r = requests.get(url_para)\n",
    "\n",
    "print(r.url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[6장: 239페이지]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests  \n",
    "\n",
    "where_value = 'nexearch' \n",
    "sm_value = 'top_hty'\n",
    "fbm_value = 1\n",
    "ie_value = 'utf8'\n",
    "query_value = 'python'\n",
    "\n",
    "url = \"https://search.naver.com/search.naver\"\n",
    "parameters = {\"where\":where_value, \"sm\":sm_value, \"fbm\":fbm_value, \"ie\":ie_value, \"query\":query_value}\n",
    "r = requests.get(url, params=parameters)\n",
    "print(r.url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 웹 사이트에서 데이터 가져오기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.1 날씨 정보 가져오기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 웹 사이트 분석해 날씨 정보 가져오기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[6장: 241페이지]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests  \n",
    "from bs4 import BeautifulSoup \n",
    "\n",
    "location = \"서울시 종로구 청운동\"\n",
    "search_query = location + \" 날씨\"\n",
    "search_url = \"https://search.daum.net/search?w=tot&DA=YZR&t__nil_searchbox=btn&sug=&sugo=&sq=&o=&q=\"\n",
    "url = search_url + search_query\n",
    "\n",
    "html_weather = requests.get(url).text\n",
    "soup_weather = BeautifulSoup(html_weather, \"lxml\")\n",
    "print(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[6장: 243페이지]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_temp = soup_weather.select_one('strong.txt_temp').get_text()\n",
    "txt_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_weather = soup_weather.select_one('span.txt_weather').get_text()\n",
    "txt_weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_weather_dds = soup_weather.select('dl.dl_weather dd')\n",
    "dl_weather_dds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[6장: 244페이지]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[wind_speed, humidity, pm10] = [x.get_text() for x in dl_weather_dds]\n",
    "\n",
    "print(f\"현재 풍속: {wind_speed}, 현재 습도: {humidity}, 미세 먼지: {pm10}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests  \n",
    "from bs4 import BeautifulSoup \n",
    "import time\n",
    "\n",
    "def get_weather_daum(location):\n",
    "    search_query = location + \" 날씨\"\n",
    "    search_url = \"https://search.daum.net/search?w=tot&DA=YZR&t__nil_searchbox=btn&sug=&sugo=&sq=&o=&q=\"\n",
    "    url = search_url + search_query\n",
    "    html_weather = requests.get(url).text\n",
    "    time.sleep(2)    \n",
    "    soup_weather = BeautifulSoup(html_weather, \"lxml\")\n",
    "    \n",
    "    txt_temp = soup_weather.select_one('strong.txt_temp').get_text()\n",
    "    txt_weather = soup_weather.select_one('span.txt_weather').get_text()\n",
    "\n",
    "    dl_weather_dds = soup_weather.select('dl.dl_weather dd')\n",
    "    [wind_speed, humidity, pm10] = [x.get_text() for x in dl_weather_dds]\n",
    "    \n",
    "    return (txt_temp, txt_weather, wind_speed, humidity, pm10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location = \"서울시 종로구 청운동\" # 날씨를 알고 싶은 지역 \n",
    "get_weather_daum(location)        # 함수 호출"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[6장: 245페이지]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location = \"경기도 수원시\" # 날씨를 알고 싶은 지역 \n",
    "\n",
    "(txt_temp, txt_weather, wind_speed, humidity, pm10) = get_weather_daum(location)\n",
    "\n",
    "print(\"-------[오늘의 날씨 정보] (Daum) ----------\")\n",
    "print(f\"- 설정 지역: {location}\")\n",
    "print(f\"- 기온: {txt_temp}\")\n",
    "print(f\"- 날씨 정보: {txt_weather} \", )\n",
    "print(f\"- 현재 풍속: {wind_speed}, 현재 습도: {humidity}, 미세 먼지: {pm10}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 날씨 정보 주기적으로 가져오기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[6장: 245페이지]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import schedule\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# 작업을 위한 함수 지정\n",
    "def job():\n",
    "    now = datetime.now()\n",
    "    print(\"[작업 수행 시각] {:%H:%M:%S}\".format(now))\n",
    "    location = \"경기도 수원시\" # 날씨를 알고 싶은 지역 \n",
    "\n",
    "    (txt_temp, txt_weather, wind_speed, humidity, pm10) = get_weather_daum(location)\n",
    "\n",
    "    print(\"-------[오늘의 날씨 정보] (Daum) ----------\")\n",
    "    print(f\"- 설정 지역: {location}\")\n",
    "    print(f\"- 기온: {txt_temp}\")\n",
    "    print(f\"- 날씨 정보: {txt_weather} \", )\n",
    "    print(f\"- 현재 풍속: {wind_speed}, 현재 습도: {humidity}, 미세 먼지: {pm10}\")\n",
    "\n",
    "# 코드 테스트를 위해 5초마다 날씨 정보 가져와 출력하기 위한 스케줄 설정\n",
    "schedule.every(5).seconds.do(job)  # 5초(second)마다 job() 함수 실행\n",
    "\n",
    "# -- 매일 지정한 시각에 날씨 정보를 가져와 출력하기 위한 스케줄 설정\n",
    "# schedule.every().day.at(\"07:00\").do(job) # 매일 07시에 job() 함수 실행\n",
    "# schedule.every().day.at(\"12:00\").do(job) # 매일 12시에 job() 함수 실행\n",
    "# schedule.every().day.at(\"18:00\").do(job) # 매일 18시에 job() 함수 실행\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        schedule.run_pending()\n",
    "        time.sleep(1)\n",
    "    except:\n",
    "        print(\"작업 강제 종료\")\n",
    "        schedule.clear()  # 기본 스케줄러 객체를 제거          \n",
    "        break            # while 문을 빠져 나옴"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.2 주식 정보 가져오기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 주식 현재가 가져오기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[6장: 248페이지]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "base_url = 'https://finance.naver.com/item/main.nhn'\n",
    "stock_code = \"005930\"\n",
    "url = base_url + \"?code=\" + stock_code\n",
    "\n",
    "html = requests.get(url).text\n",
    "soup = BeautifulSoup(html, 'lxml')\n",
    "\n",
    "print(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[6장: 250페이지]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "soup.select_one('p.no_today')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_price = soup.select_one('p.no_today span.blind').get_text()\n",
    "stock_price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def get_current_stock_price(stock_code):\n",
    "    \n",
    "    base_url = 'https://finance.naver.com/item/main.nhn'\n",
    "    url = base_url + \"?code=\" + stock_code\n",
    "    \n",
    "    html = requests.get(url).text\n",
    "    soup = BeautifulSoup(html, 'lxml')\n",
    "    \n",
    "    stock_price = soup.select_one('p.no_today span.blind').get_text()\n",
    "    \n",
    "    return stock_price"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[6장: 251페이지]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_code = \"005930\"\n",
    "current_stock_price = get_current_stock_price(stock_code)\n",
    "current_stock_price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "company_stock_codes = {\"삼성전자\": \"005930\", \"현대차\":\"005380\", \"NAVER\":\"035420\"}\n",
    "\n",
    "print(\"[현재 주식 가격(원)]\")\n",
    "for company, stock_code in company_stock_codes.items():\n",
    "    current_stock_price = get_current_stock_price(stock_code)\n",
    "    print(f\"{company}: {current_stock_price}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 주식 종목 코드 가져오기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[6장: 252페이지]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 한국 거래소(KRX)에서 전체 상장법인 목록 가져오기\n",
    "base_url = \"http://kind.krx.co.kr/corpgeneral/corpList.do\"\n",
    "method = \"download\"\n",
    "url = \"{0}?method={1}\".format(base_url, method)\n",
    "\n",
    "df = pd.read_html(url, header=0)[0]\n",
    "\n",
    "with pd.option_context('display.max_columns',4): # 최대 4개까지 열이 표시하도록 설정\n",
    "    pd.set_option(\"show_dimensions\", False)      # 행과 열 개수 출력 안 하기\n",
    "    display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['종목코드']= df['종목코드'].apply(lambda x: f\"{x:06d}\")\n",
    "\n",
    "with pd.option_context('display.max_columns',4): # 최대 4개까지 열이 표시하도록 설정\n",
    "    pd.set_option(\"show_dimensions\", False)      # 행과 열 개수 출력 안 하기\n",
    "    display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = df[['회사명','종목코드']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[6장: 254페이지]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#----------------------------------------------------\n",
    "# 한국 주식의 종목 이름과 종목 코드를 가져오는 함수\n",
    "#----------------------------------------------------\n",
    "def get_stock_info(maket_type=None):\n",
    "    # 한국거래소(KRX)에서 전체 상장법인 목록 가져오기\n",
    "    base_url =  \"http://kind.krx.co.kr/corpgeneral/corpList.do\"\n",
    "    method = \"download\"\n",
    "    if maket_type == 'kospi':\n",
    "        marketType = \"stockMkt\"  # 주식 종목이 코스피인 경우\n",
    "    elif maket_type == 'kosdaq':\n",
    "        marketType = \"kosdaqMkt\" # 주식 종목이 코스닥인 경우\n",
    "    elif maket_type == None:\n",
    "        marketType = \"\"\n",
    "    url = \"{0}?method={1}&marketType={2}\".format(base_url, method, marketType)\n",
    "\n",
    "    df = pd.read_html(url, header=0)[0]\n",
    "    \n",
    "    # 종목코드 열을 6자리 숫자로 표시된 문자열로 변환\n",
    "    df['종목코드']= df['종목코드'].apply(lambda x: f\"{x:06d}\") \n",
    "    \n",
    "    # 회사명과 종목코드 열 데이터만 남김\n",
    "    df = df[['회사명','종목코드']]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_kospi = get_stock_info('kospi')\n",
    "df_kospi.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[6장: 255페이지]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_kosdaq = get_stock_info('kosdaq')\n",
    "df_kosdaq.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------------------------\n",
    "# 회사 이름을 입력하면 종목 코드를 가져오는 함수\n",
    "#--------------------------------------------------\n",
    "def get_stock_code(company_name, maket_type=None):\n",
    "    df = get_stock_info(maket_type)\n",
    "    code = df[df['회사명']==company_name]['종목코드'].values\n",
    "    \n",
    "    if(code.size !=0):\n",
    "        code = code[0]    \n",
    "        return code\n",
    "    else:\n",
    "        print(f\"[Error]입력한 [{company_name}]에 대한 종목 코드가 없습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_stock_code('삼성전자', 'kospi') # 삼성전자 주식 종목 코드 가져오기, 코스피(kospi) 지정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[6장: 256페이지]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_stock_code('삼성전자') # 삼성전자 주식 종목 코드 가져오기, 주식 종류는 지정 안 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_stock_code('현대차')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_stock_code('현대자동차')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "company_names = [\"삼성전자\", \"현대자동차\", \"NAVER\"]\n",
    "\n",
    "print(\"[현재 주식 가격(원)]\")\n",
    "for company_name in company_names:\n",
    "    stock_code = get_stock_code(company_name)\n",
    "    current_stock_price = get_current_stock_price(stock_code)\n",
    "    print(f\"{company_name}: {current_stock_price}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[6장: 257페이지]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_stock_code('CJ 바이오사이언스', 'kosdaq') # 주식 종목 코드 가져오기, 코스닥(kosdaq) 지정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_stock_code('CJ 바이오사이언스') # 주식 종목 코드 가져오기, 주식 종류는 지정 안 함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.3 환율 정보 가져오기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 현재의 환율 정보 가져오기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[6장: 259페이지]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "url = 'https://search.naver.com/search.naver?where=nexearch&sm=top_hty&fbm=1&ie=utf8&query=%ED%99%98%EC%9C%A8'\n",
    "\n",
    "# url에서 표 데이터를 추출해 DataFrame 데이터의 리스트로 반환\n",
    "dfs = pd.read_html(url)\n",
    "dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[6장: 260페이지]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "exchange_rate_df = dfs[0].replace({'전일대비상승': '▲', \n",
    "                                   '전일대비하락': '▼'}, regex=True)\n",
    "exchange_rate_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[6장: 262페이지]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 네이버 금융의 환율 정보 웹 사이트 주소\n",
    "url = 'https://finance.naver.com/marketindex/exchangeList.nhn' \n",
    "\n",
    "# 웹 사이트의 표 데이터에서 두 번째 줄을 DataFrame 데이터의 columns로 선택\n",
    "dfs = pd.read_html(url, header=1) \n",
    "\n",
    "dfs[0].head() # 전체 데이터 중 앞의 일부분만 표시"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 과거의 환율 정보 가져오기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[6장: 264페이지]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "base_url = \"https://finance.naver.com/marketindex/exchangeDailyQuote.nhn\"\n",
    "currency_code = \"FX_USDKRW\" # 통화 코드\n",
    "page_num = 1\n",
    "\n",
    "url = f\"{base_url}?marketindexCd={currency_code}&page={page_num}\"\n",
    "dfs = pd.read_html(url, header=1)\n",
    "\n",
    "# 행과 열의 최대 표시 개수를 임시로 설정\n",
    "with pd.option_context('display.max_rows',4, 'display.max_columns',6): \n",
    "    pd.set_option(\"show_dimensions\", False) # 행과 열 개수 정보 숨기기\n",
    "    display(dfs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[6장: 265페이지]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# 날짜별 환율 데이터를 반환하는 함수\n",
    "# - 입력 인수: currency_code(통화코드), last_page_num(페이지 수)\n",
    "# - 반환: 환율 데이터\n",
    "def get_exchange_rate_data(currency_code, last_page_num):\n",
    "    base_url = \"https://finance.naver.com/marketindex/exchangeDailyQuote.nhn\"\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "    for page_num in range(1, last_page_num+1):\n",
    "        url = f\"{base_url}?marketindexCd={currency_code}&page={page_num}\"\n",
    "        dfs = pd.read_html(url, header=1)\n",
    "\n",
    "        if dfs[0].empty: # 통화 코드가 잘못 지정됐거나 마지막 페이지의 경우 for 문을 빠져나오기 위한 코드\n",
    "            if (page_num==1):\n",
    "                print(f\"통화 코드({currency_code})가 잘못 지정됐습니다.\")\n",
    "            else:\n",
    "                print(f\"{page_num}가 마지막 페이지입니다.\")\n",
    "            break\n",
    "\n",
    "        df = pd.concat([df, dfs[0]], ignore_index=True) # page별로 가져온 DataFrame 데이터 연결\n",
    "        time.sleep(0.1) # 0.1초간 멈춤\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[6장: 266페이지]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_usd = get_exchange_rate_data('FX_USDKRW', 2)\n",
    "\n",
    "# 행과 열의 최대 표시 개수를 임시로 설정\n",
    "with pd.option_context('display.max_rows',4, 'display.max_columns',6):\n",
    "    pd.set_option(\"show_dimensions\", False) # 행과 열 개수 정보 숨기기\n",
    "    display(df_usd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[6장: 267페이지]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_eur = get_exchange_rate_data('FX_EURKRW', 1)\n",
    "\n",
    "# 행과 열의 최대 표시 개수를 임시로 설정\n",
    "with pd.option_context('display.max_rows',4, 'display.max_columns',6):\n",
    "    pd.set_option(\"show_dimensions\", False) # 행과 열 개수 정보 숨기기\n",
    "    display(df_eur.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.4 부동산 정보 가져오기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[6장: 269페이지]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "base_url = \"https://land.naver.com/news/trendReport.naver\"\n",
    "page_num = 1\n",
    "\n",
    "url = f\"{base_url}?page={page_num}\"\n",
    "dfs = pd.read_html(url)\n",
    "\n",
    "df = dfs[0] # 리스트의 첫 번째 항목에 동향 보고서 제목 데이터가 있음\n",
    "\n",
    "# 행과 열의 최대 표시 개수를 임시로 설정\n",
    "with pd.option_context('display.max_rows',4, 'display.max_columns',6): \n",
    "    pd.set_option(\"show_dimensions\", False) # 행과 열 개수 정보 숨기기\n",
    "    display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[6장: 270페이지]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 원본 DataFrame의 제목 열에 있는 문자열을 분리해 \n",
    "# 전국, 서울, 수도권의 매매가 변화율 열이 있는 DataFrame 반환하는 함수\n",
    "def split_title_to_rates(df_org):\n",
    "    df_new = df_org.copy()\n",
    "\n",
    "    df_temp = df_new['제목'].str.replace('%', '') # 제목 문자열에서 % 제거\n",
    "    df_temp = df_temp.str.replace('보합', '0')    # 제목 문자열에서 보합을 0으로 바꿈\n",
    "    df_temp = df_temp.str.replace('보합세', '0')  # 제목 문자열에서 보합세를 0으로 바꿈\n",
    "    \n",
    "    regions = ['전국', '서울', '수도권']    \n",
    "    for region in regions:\n",
    "        df_temp = df_temp.str.replace(region, '') # 문자열에서 전국, 서울, 수도권 제거\n",
    "\n",
    "    df_temp = df_temp.str.split(']', expand=True) # ]를 기준으로 열 분리\n",
    "    df_temp = df_temp[1].str.split(',', expand=True) # ,를 기준으로 열 분리\n",
    "    \n",
    "    df_temp = df_temp.astype(float)\n",
    "    \n",
    "    df_new[regions] = df_temp # 전국, 서울, 수도권 순서대로 DataFrame 데이터에 할당\n",
    "\n",
    "    return df_new[['등록일'] + regions + ['번호']] # DataFrame에서 필요한 열만 반환"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[6장: 271페이지]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rate = split_title_to_rates(df) # split_title_to_rates() 함수 호출\n",
    "df_rate.head()                     # 앞의 일부만 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "base_url = \"https://land.naver.com/news/trendReport.naver\"\n",
    "\n",
    "df_rates = pd.DataFrame() # 전체 데이터가 담길 DataFrame 데이터\n",
    "last_page_num = 8 # 가져올 데이터의 마지막 페이지 \n",
    "\n",
    "for page_num in range(1, last_page_num+1):\n",
    "\n",
    "    url = f\"{base_url}?page={page_num}\"\n",
    "    dfs = pd.read_html(url)\n",
    "\n",
    "    df_page = dfs[0] # 리스트의 첫 번째 항목에 동향 보고서 제목 데이터가 있음\n",
    "    df_rate = split_title_to_rates(df_page)\n",
    "    \n",
    "    # 세로 방향으로 연결 (기존 index를 무시)\n",
    "    df_rates = pd.concat([df_rates, df_rate], ignore_index=True) \n",
    "\n",
    "# 최신 데이터와 과거 데이터의 순서를 바꿈. index도 초기화함  \n",
    "df_rates = df_rates[::-1].reset_index(drop=True)\n",
    "df_rates.head() # 앞의 일부만 출력\n",
    "\n",
    "# 행과 열의 최대 표시 개수를 임시로 설정\n",
    "with pd.option_context('display.max_rows',4, 'display.max_columns',6): \n",
    "    pd.set_option(\"show_dimensions\", False) # 행과 열 개수 정보 숨기기\n",
    "    display(df_rates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[6장: 273페이지]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "\n",
    "mpl.rcParams['font.family'] = 'Malgun Gothic' # '맑은 고딕'으로 폰트 설정 \n",
    "mpl.rcParams['axes.unicode_minus'] = False # 마이너스(-) 폰트 깨짐 방지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df_rates.tail(40).plot(x='등록일', y=['전국', '서울', '수도권'], \n",
    "                       figsize=(10, 8), subplots=True, layout=(3,1),\n",
    "                       style = '-o', grid=True) # 그래프 그리기\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.5 웹 페이지에서 이미지 가져오기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[6장: 275페이지]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests  \n",
    "\n",
    "image_url = 'https://www.python.org/static/img/python-logo.png' # 이미지 링크(주소)\n",
    "r = requests.get(image_url) # 이미지 주소의 HTTP 응답 객체\n",
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[6장: 276페이지]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = image_url.split(\"/\")[-1]\n",
    "file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "download_folder = 'C:/myPyScraping/data/ch06/download' \n",
    "image_dir_path = Path(download_folder)\n",
    "\n",
    "if not image_dir_path.exists():\n",
    "    image_dir_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "print(\"생성한 폴더:\", download_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[6장: 277페이지]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = image_dir_path/file_name\n",
    "image_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[6장: 278페이지]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get(image_url) # 이미지 주소의 HTTP 응답 객체\n",
    "image_data = r.content # 응답 객체(r)을 이용해 받은 이미지 데이터\n",
    "\n",
    "with open(image_path, 'wb') as f:\n",
    "    f.write(image_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "from pathlib import Path\n",
    "\n",
    "# Unsplash의 사진 이미지 주소\n",
    "image_url = \"https://images.unsplash.com/photo-1645956734658-8b6e62e7d35a\"\n",
    "  \n",
    "file_name = image_url.split(\"/\")[-1] + \".jpg\" # 파일 이름 생성(확장자 추가)\n",
    "download_folder = 'C:/myPyScraping/data/ch06/download' # 다운로드 폴더 지정\n",
    "\n",
    "# 지정한 다운로드 폴더를 생성하지 않았으면 생성\n",
    "image_dir_path = Path(download_folder)\n",
    "if not image_dir_path.exists():\n",
    "    image_dir_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "image_path = image_dir_path/file_name # 전체 경로(폴더 + 파일명)\n",
    "\n",
    "r = requests.get(image_url, stream=True)\n",
    "if r.status_code == 200:  \n",
    "    with open(image_path, 'wb') as f:\n",
    "        for chunk in r.iter_content(1024):\n",
    "            f.write(chunk)\n",
    "    print(\"- 이미지를 다운로드했습니다\")\n",
    "else:\n",
    "    print(\"- 지정한 이미지 링크의 응답이 없습니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 정리"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nav_menu": {},
  "toc": {
   "base_numbering": "6",
   "nav_menu": {
    "height": "400px",
    "width": "252px"
   },
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "545px",
    "left": "46px",
    "right": "1154px",
    "top": "142.133px",
    "width": "271px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "toc_position": {
   "height": "503px",
   "left": "0px",
   "right": "952.167px",
   "top": "107px",
   "width": "300px"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
